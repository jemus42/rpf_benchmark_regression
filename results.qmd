---
title: "RPF Regression Benchmark Results"
author: "Lukas"
date: now
date-format: "YYYY-MM-DD HH:mm z"
date-modified: last-modified
lightbox: true
format: 
  html:
    code-fold: true
    fig-align: center
    toc: true
    theme:
      light: flatly
      dark: darkly
  pdf:
    documentclass: article
    fig-align: center
    toc: true
    date: now
    keep-tex: true
    knitr:
      opts_chunk:
        echo: false
        fig.pos: H
    # include-in-header:
    #   - \usepackage{float}
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| message: false
#| warning: false

library(ggplot2)
library(ragg)
library(data.table)
library(dplyr)
library(kableExtra)

# conf <- config::get(config = "production")
jobs <- readRDS(fs::path(conf$result_path, "jobs", ext = "rds"))

# Load task metadata
task_meta <- readRDS("task_meta.rds")

task_meta <- task_meta |>
  arrange(dim_rank) |>
  mutate(
    task_label = glue::glue("{task_id} ({n} x {p})"),
    task_label = factor(task_label, levels = task_label, ordered = TRUE),
    task_id = factor(task_id, levels = task_id, ordered = TRUE)
  )

# benchmark results
aggr   <- readRDS(fs::path(conf$result_path, "aggr",   ext = "rds"))
scores <- readRDS(fs::path(conf$result_path, "scores", ext = "rds"))
aggr   <- task_meta[aggr, on = c("task_id")]
scores <- task_meta[scores, on = c("task_id")]

archives_rpf <- readRDS(fs::path(conf$result_path, "archives_rpf", ext = "rds"))
archives_rpf_fixdepth <- readRDS(fs::path(conf$result_path, "archives_rpf_fixdepth", ext = "rds"))
archives_rpf = rbind(archives_rpf, archives_rpf_fixdepth)

results_rpf <- readRDS(fs::path(conf$result_path, "results_rpf", ext = "rds"))
results_rpf_fixdepth <- readRDS(fs::path(conf$result_path, "results_rpf_fixdepth", ext = "rds"))
results_rpf = rbind(results_rpf, results_rpf_fixdepth)


archives_xgb <- readRDS(fs::path(conf$result_path, "archives_xgb", ext = "rds"))
archives_xgb_fixdepth <- readRDS(fs::path(conf$result_path, "archives_xgb_fixdepth", ext = "rds"))
archives_xgb = rbind(archives_xgb, archives_xgb_fixdepth)
archives_ranger <- readRDS(fs::path(conf$result_path, "archives_ranger", ext = "rds"))

runtimes <- readRDS(fs::path(conf$result_path, "runtimes", ext = "rds"))


# Define learner colors for somewhat identifiable plots
learner_cols <- c(
  "rpf" = "#F73098",
  "xgb" = "#3BA99C",
  "rpf_fixdepth" = "#CA1694",
  "xgb_fixdepth" = "#256A62",
  "ranger" = "#2171B5",
  "featureless" = "#484848"
)
learner_order <- names(learner_cols)
learner_label <- function(x) {
  x <- as.character(x)
  ret <- c("rpf" = "rpf", "xgb" = "xgboost", "rpf_fixdepth" = "rpf (2)", "xgb_fixdepth" = "xgboost (2)", "ranger" = "rf", "featureless" = "average")[x]
  unname(ret)
}
learner_order_labelled <- learner_label(learner_order)

# aggr[, learner_label := learner_label(learner_id)]
# scores[, learner_label := learner_label(learner_id)]
aggr[, learner_id := factor(learner_id, levels = rev(learner_order))]
scores[, learner_id := factor(learner_id, levels = rev(learner_order))]

set.seed(3) # Only for jitterdodge consistency
```

```{r task-subselection, eval=FALSE}
tasks_keep <- as.character(task_meta[dim_rank <= 12, task_id])

task_meta <- task_meta[task_id %in% tasks_keep, ]
scores <- scores[task_id %in% tasks_keep, ]
aggr <- aggr[task_id %in% tasks_keep, ]

results_rpf <- results_rpf[task_id %in% tasks_keep, ]
archives_rpf <- archives_rpf[task_id %in% tasks_keep, ]
archives_ranger <- archives_ranger[task_id %in% tasks_keep, ]
archives_xgb <- archives_xgb[task_id %in% tasks_keep, ]
```


## Benchmark Setup

-   Inner resampling: 2x3-fold repeated CV
-   Outer resampling: 10-fold CV using the same resampling folds as [the CTR-23 paper](https://openreview.net/pdf?id=HebAOoMm94).
      - Tasks use 10x10-fold CV if N <= 1000, 10-fold CV otherwise
-   Bayesian optimization with 200 evaluations
-   A runtime limit of 7 days for each outer resampling fold (including tuning, which is limited to 6 days)

### Status

The number of completed jobs per learner.

```{r job-status}
jobs[, .(done = sum(!is.na(time.running)), total = .N), by = .(learner_id)]  |>
  mutate(perc = scales::percent(done/total)) |>
  arrange(done) |>
  knitr::kable()
```

```{r jobs-completion-history}
#| fig-width: 10
#| fig-height: 7
#| fig-align: center

jobs |>
  arrange(done) |>
  group_by(learner_id) |>
  mutate(
    cumdone = cumsum(!is.na(time.running)),
    cumdone_rel = cumdone/n()
  ) |>
  ungroup() |>
  filter(!is.na(time.running)) |>
  # mutate(
  #   time_rel = as.numeric(difftime(done, min(done), units = "days"))
  # ) |>
  #select(learner_id, done, time_rel)
  ggplot(aes(x = done, y = cumdone_rel, color = learner_id)) +
  geom_step(linewidth = 1.5, key_glyph = "rect") +
  scale_x_datetime(
    date_breaks = "7 days",
    date_minor_breaks = "1 day",
    date_labels = "%b %d",
    sec.axis = sec_axis(
      transform = \(x) as.numeric(difftime(x, min(x), units = "days")),
      labels = as.numeric, 
      name = "Days since start"
    )
  ) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_manual(breaks = learner_order, values = learner_cols, labels = learner_label) +
  labs(
    title = "Regression Benchmark Completion Rate",
    subtitle = "Date of completion and days since benchmark start",
    x = "Time of Completion", y = "Completion Rate",
    color = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    plot.title.position = "plot"
  )
```

### Hyperparameter Search Spaces

#### RPF / RPF (2)

-   `ntrees` := 50
-   `max_interaction` in \[1, 20\]
    -   Via `max_interaction_ratio` in \[0, 1\] and `max_interaction_limit` := 20
    -   for **RPF (2)**: `max_interaction` := 2
-   `splits` in \[10, 200\]
-   `split_try` in \[1, 20\]
-   `t_try` in \[0.1, 1\]

#### ranger

-   `num.trees` := 500
-   `mtry.ratio` in \[0.1, 1\]
-   `min.node.size` in \[1, 50\]
-   `sample.fraction` in \[0.1, 1\]

#### XGBoost / XGBoost (2)

-   Preprocessing of categorical features using treatment encoding
-   `max_depth` in \[1, 20\]
    -   for **XGBoost (2)**: `max_depth` := 2
-   `subsample` in \[0.1, 1\]
-   `colsample_bytree` in \[0.1, 1\]
-   `eta` in \[0.0001, 1\] (log scale)
-   `nrounds` in \[10, 5000\]

### Tasks

[OpenML CTR-23](https://www.openml.org/search?type=study&study_type=task&sort=tasks_included&id=353) tasks with:

-   No missing values
-   $n \cdot p \leq 10^6$
-   No features of type `logical` or `character`

```{r tasks}
task_meta |>
  arrange(dim) |>
  select(task_id, n, p) |>
  kbl(
    caption = "Selected OpenML CTR-23 tasks, arranged by dimensionality",
    col.names = c("Task", "n", "p"),
    booktabs = TRUE
  ) |>
  kable_styling()
```

#### Completion by task

```{r tbl-task-completion}
task_completion <- jobs |>
  filter(learner_id != "featureless") |>
  mutate(is_done = !is.na(time.running)) |>
  summarize(
    perc_done = sum(is_done)/n(), 
    .by = c("task_id", "learner_id")
  ) |>
  tidyr::pivot_wider(id_cols = c("task_id"), names_from = "learner_id", values_from = "perc_done") |>
  mutate(
    total = (rpf + rpf_fixdepth + xgb + xgb_fixdepth + ranger)/5
  ) |>
  left_join(task_meta[, .(task_id, n, p)], by = "task_id") |>
  arrange(n * p)

task_completion |>
  mutate(across(rpf:total, scales::percent)) |>
  kableExtra::kbl(caption = "Proportion of completed jobs by learner and task") |>
  kableExtra::kable_styling()
```

```{r tbl-task-completion-completed}
completed_tasks <- task_completion |>
  filter(total == 1)

task_completion |>
  filter(total < 1) |>
  mutate(across(rpf:total, scales::percent)) |>
  kableExtra::kbl(caption = "Tasks with incomplete results") |>
  kableExtra::kable_styling()

# task_meta <- task_meta[task_id %in% completed_tasks$task_id, ]
# scores <- scores[task_id %in% completed_tasks$task_id, ]
# aggr <- aggr[task_id %in% completed_tasks$task_id, ]
```


## Regression Benchmark Results

-   Tuning on Mean Squared Error (MSE)
-   Evaluation on
    -   Root Mean Square Error (RMSE)
    -   Root Relative Squared Error ([RRSE](https://mlr3measures.mlr-org.com/reference/rrse.html)), where 1 corresponds to the performance of the featureless learner and lower is better
    -   Median Absolute Error (MAE)

### Aggregated Results

```{r fun-plot_aggr}
plot_aggr <- function(xdf, 
                      measures = c("rmse", "mae", "rrse"),
                      include_featless = TRUE, logscale = FALSE, ncol = 2, outlier_threshold = 1e4) {
  
  xdf <- xdf |>
    tidyr::pivot_longer(
      cols = any_of(measures),
      names_to = "measure",
      values_to = "score"
    ) |>
    filter(!(measure == "rrse" & learner_id == "featureless")) |>
    mutate(
      measure = factor(measure, levels = c("rmse", "mae", "rrse"))
    )

  if (!include_featless) {
    xdf <- xdf |>
      filter(learner_id != "featureless")
  }
  
  if (!logscale) {
    num_outliers <- sum(xdf$score > outlier_threshold)
    xdf <- xdf |>
      filter(score < .env$outlier_threshold)
  }

  p <- xdf |>
    filter(.data$measure %in% .env$measures) |>
    ggplot(aes(y = learner_id, x = score, fill = learner_id)) +
    facet_wrap(vars(measure), ncol = ncol, scales = "free", labeller = \(x) lapply(x, toupper)) +
    geom_boxplot(alpha = .5) +
    geom_point(
      position = position_jitterdodge(dodge.width = .25),
      shape = 21
    ) +
    scale_y_discrete(labels = learner_label) +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      #title = "Aggregated scores over all tasks",
      x = NULL, y = NULL
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title.position = "plot"
    )
  
  if (logscale) {
    p <- p + scale_x_log10(labels = scales::label_comma(accuracy = 0.01)) +
      labs(x = "Scores (log10)")
  } else {
    p <- p + scale_x_continuous(labels = scales::label_comma()) +
      labs(x = "Scores")
    
    if (!identical(measures, "rrse")) {
      p <- p + labs(caption = glue::glue("{num_outliers} outlier scores above {outlier_threshold} are omitted"))
    }
  }
  p
}
```

#### Boxplot

Mixed scales depending on measure

```{r aggregated-hybrid-rmse-mae}
#| fig-width: 7
#| fig-height: 4
#| fig-align: center

library(patchwork)
# plot_aggr(aggr, measures = "rmse", include_featless = TRUE, logscale = TRUE, ncol = 1) /
#   plot_aggr(aggr, measures = "mae", include_featless = TRUE, logscale = TRUE, ncol = 1) /
#   plot_aggr(aggr, measures = "rrse", include_featless = FALSE, logscale = FALSE, ncol = 1)

xtemp <- aggr |>
    tidyr::pivot_longer(
      cols = any_of(c("rmse", "mae", "rrse")),
      names_to = "measure",
      values_to = "score",
      names_transform = toupper
    ) |>
    filter(!(measure == "RRSE" & learner_id == "featureless"))

p1 <- xtemp |>
    filter(measure == "RMSE") |>
    ggplot(aes(y = learner_id, x = score, fill = learner_id)) +
    geom_boxplot(alpha = .5) +
    geom_point(
      position = position_jitterdodge(dodge.width = .5),
      shape = 21, stroke = .1, size = 2
    ) +
    scale_x_log10(guide = guide_axis(position = "top"), labels = scales::label_comma(accuracy = 0.01)) +
    scale_y_discrete(labels = learner_label) +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      #title = "Aggregated scores over all tasks",
      x = "RMSE (log10)", y = NULL
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title.position = "plot"
    )

p2 <- xtemp |>
    filter(measure == "MAE") |>
    ggplot(aes(y = learner_id, x = score, fill = learner_id)) +
    geom_boxplot(alpha = .5) +
    geom_point(
      position = position_jitterdodge(dodge.width = .5),
      shape = 21, stroke = .1, size = 2
    ) +
    scale_x_log10(guide = guide_axis(position = "top"), labels = scales::label_comma(accuracy = 0.01)) +
    scale_y_discrete(labels = learner_label) +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      #title = "Aggregated scores over all tasks",
      x = "MAE (log10)", y = NULL
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title.position = "plot"
    )

p1 / p2

```


```{r aggregated-rrse}
#| fig-width: 7
#| fig-height: 1.75
#| fig-align: center

p3 <- xtemp |>
    filter(measure == "RRSE") |>
    ggplot(aes(y = learner_id, x = score, fill = learner_id)) +
    geom_boxplot(alpha = .5) +
    geom_point(
      position = position_jitterdodge(dodge.width = .5),
      shape = 21, stroke = .1, size = 2
    ) +
    scale_x_continuous(
      guide = guide_axis(position = "top"), 
      labels = scales::label_percent(accuracy = 1), 
      limits = c(0, 1)
    ) +
    scale_y_discrete(labels = learner_label) +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      #title = "Aggregated scores over all tasks",
      x = "RRSE (%)", y = NULL
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title.position = "plot"
    )

p3
```


```{r aggregated-hybrid-combined, eval=FALSE}
#| fig-width: 7
#| fig-height: 5
#| fig-align: center
p1 / p2 / p3
```

<!-- ::: -->

#### Table

<!--
```{r aggregated-table-meanmedian}
# column: screen-inset-shaded
aggr |>
  tidyr::pivot_longer(
    cols = any_of(c("rmse", "mae", "rrse")),
    names_to = "measure", names_transform = toupper,
    values_to = "score"
  ) |>
  group_by(learner_id, measure) |>
  summarize(
    mean = mean(score, na.rm = TRUE),
    median = median(score, na.rm = TRUE),
    sd = sd(score, na.rm = TRUE),
    q25 = quantile(score, .25, na.rm = TRUE),
    q75 = quantile(score, .75, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(across(where(is.numeric), \(x) round(x, 2))) |>
  mutate(
    mean = glue::glue("{mean} [{sd}]"),
    median = glue::glue("{median} [{q25}; {q75}]")
  ) |>
  select(-sd, -q25, -q75) |>
  tidyr::pivot_wider(names_from = measure, names_sep = "_", values_from = mean:median) |>
  select(learner_id, contains(toupper(c("rmse", "mae", "rrse")))) |>
  arrange(desc(learner_id)) |>
  mutate(learner_id = learner_label(learner_id)) |>
  kbl(
    col.names = c("Learner", rep(c("Mean [SD]", "Median [q25; q75]"), 3)),
    booktabs = TRUE
  ) |>
  kable_styling(full_width = TRUE, latex_options = c("HOLD_position")) |>
  add_header_above(header = c(" " = 1, "RMSE" = 2, "MAE" = 2, "RRSE" = 2)) |>
  column_spec(1, "2.5cm")
```
-->

```{r aggregated-table-median}
aggr_tab <- aggr |>
  mutate(
    rrse = if_else(learner_id == "featureless", 100, 100 * rrse)
  ) |>
  tidyr::pivot_longer(
    cols = any_of(c("rmse", "mae", "rrse")),
    names_to = "measure", names_transform = toupper,
    values_to = "score"
  ) |>
  group_by(learner_id, measure) |>
  summarize(
    #mean = mean(score, na.rm = TRUE),
    median = median(score, na.rm = TRUE),
    #sd = sd(score, na.rm = TRUE),
    q25 = quantile(score, .25, na.rm = TRUE),
    q75 = quantile(score, .75, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(across(where(is.numeric), \(x) round(x, 2))) |>
  mutate(
    #mean = glue::glue("{mean} [{sd}]"),
    median = glue::glue("{median} [{q25}; {q75}]")
  ) |>
  select(-q25, -q75) |>
  tidyr::pivot_wider(names_from = measure, names_sep = "_", values_from = median)

aggr_tab |>
  select(learner_id, contains(toupper(c("rmse", "mae", "rrse")))) |>
  arrange(desc(learner_id)) |>
  mutate(learner_id = learner_label(learner_id)) |>
  kbl(
    col.names = c("Learner", "RMSE", "MAE", "RRSE (%)"),
    caption = "Median [q25, q75] of scores over all datasets and outer resampling iterations",
    booktabs = TRUE,
  ) |>
  kable_styling(latex_options = c("hold_position"))

aggr_tab |>
  select(learner_id, RRSE) |>
  filter(learner_id != "featureless") |>
  arrange(desc(learner_id)) |>
  mutate(learner_id = learner_label(learner_id)) |>
  kbl(
    col.names = c("Learner", "RRSE (%)"),
    caption = "Median [q25, q75] of RRSE over all datasets and outer resampling iterations",
    booktabs = TRUE,
  ) |>
  kable_styling(latex_options = c("hold_position"))
  # column_spec(1, "2.5cm")
```

### Results per Task

```{r fun-plot_scores}
plot_scores <- function(xdf, measure = "rmse", include_featless = TRUE, logscale = FALSE, ncol = 3) {
  xdf <- xdf |>
    tidyr::pivot_longer(
      cols = any_of(c("rmse", "mae", "rrse")),
      names_to = "measure", names_transform = toupper,
      values_to = "score"
    ) |>
    filter(.data$measure == toupper(.env$measure))
  
  if (!include_featless) {
    xdf <- xdf |>
      filter(learner_id != "featureless")
  }
  
  if (measure == "rrse") {
    xdf <- xdf |>
      filter(learner_id != "featureless") |>
      mutate(score = 100 * score)
    
    measure <- "RRSE (%)"
  }
  
  p <- xdf |>
    mutate(
      task_label = stringr::str_replace(task_label, "\\s\\(", "\\\n("),
      task_label = reorder(task_label, n*p)
      #task_label = stringr::str_wrap(task_label, width = 20, whitespace_only = FALSE)
    ) |>
    ggplot(aes(y = learner_id, x = score, fill = learner_id, color = learner_id)) +
    facet_wrap(vars(task_label), ncol = ncol, scales = "free") +
    geom_boxplot(alpha = .5) +
    geom_jitter(shape = 21, stroke = .5, alpha = .15, size = 1) +
    scale_fill_manual(values = learner_cols, guide = "none", aesthetics = c("color", "fill")) +
    scale_y_discrete(labels = learner_label) +
    labs(
      # title = "Scores per task",
      # subtitle = "Tasks ordered by n * p, decreasing",
      y = NULL, x = toupper(measure)
    ) +
    theme_minimal(base_size = 13) +
    theme(
      panel.spacing.x = unit(1, "cm"),
      plot.title.position = "plot"
    )
  
  if (logscale) {
    p + scale_x_log10(labels = scales::label_comma(accuracy = 0.01))
  } else {
    p + scale_x_continuous(labels = scales::label_comma())
  }
}
```

Tasks ordered by $n \cdot p$, decreasing.

::: panel-tabset
#### RMSE

```{r per-task-rmse}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#| message: false
#| error: false
# column: screen-inset-shaded

plot_scores(scores, "rmse", include_featless = TRUE, logscale = FALSE, ncol = 4)

# ggsave("tmp.png", plot = p, width = 13, height = 11, bg = "white")
```


#### MAE

```{r per-task-mae}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#| message: false
#| error: false
# column: screen-inset-shaded

plot_scores(scores, "mae", include_featless = TRUE, logscale = FALSE, ncol = 4)
```

#### RRSE

```{r per-task-rrse-no-featless}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#| message: false
#| error: false
# column: screen-inset-shaded

plot_scores(scores, "rrse", include_featless = FALSE, logscale = FALSE, ncol = 4)
```
:::

#### Runtime vs Performance

Jobs were distributed across different cluster partitions with varying performance, which makes runtimes below less comparable than desired, but some rough trends should be visible.

```{r scores-runtime-rrse}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#|
scores_runtime <- inner_join(
  scores,
  jobs[, .(task_id, learner_id, repl, time.hours)],
  by = c("task_id", "learner_id", "iteration" = "repl")
) |>
  filter(learner_id != "featureless")

scores_runtime |>
  ggplot(aes(x = time.hours, y = rrse, color = learner_id, fill = learner_id)) +
  facet_wrap(vars(task_label), scales = "free", ncol = 4) +
  geom_point(alpha = .5, key_glyph = "rect") +
  scale_x_log10() +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_manual(
    values = learner_cols, 
    labels = learner_label, 
    breaks = learner_order,
    aesthetics = c("color", "fill")) +
  labs(
    y = "RRSE (%)", x = "Runtime (hours, log10)",
    color = NULL, fill = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom"
  )
```

```{r lm, eval=FALSE}
lm(log(time.hours) ~ n:p + learner_id, data = scores_runtime) |>
  broom::tidy(exponentiate = TRUE, conf.int = TRUE)

scores_runtime |>
  summarize(
    mean = mean(time.hours), 
    median = median(time.hours), 
    sd = sd(time.hours), 
    .by = "learner_id"
  )
```


### Tuning

#### Effective Budgets

Since tuning budgets was _either_ 200 evaluations _or_ a tuning time of 6 days, the number of evaluations per learner/task varies.

Average number of evaluations for learner and task combinations, where the average is taken across resamplings:

```{r tbl-tuning-archives-eval-count}
data.table::rbindlist(
  list(
    archives_rpf[, .N, by = .(learner_id, task_id, iteration)],
    archives_ranger[, .N, by = .(learner_id, task_id, iteration)],
    archives_xgb[, .N, by = .(learner_id, task_id, iteration)]
  )
) |>
  summarize(
    n = n(),
    median = median(N),
    min_max = if_else(min(N) == max(N), "-", glue::glue("{min(N)} - {max(N)}")),
    fmt = glue::glue("{median} ({min_max})"),
    .by = c("task_id", "learner_id")
  ) |>
  arrange(task_id) |>
  left_join(
    task_meta[, .(task_id, task_label)],
    by = "task_id"
  ) |>
  mutate(
    learner_id = learner_label(learner_id),
    learner_id = factor(learner_id, levels = learner_order_labelled),
  ) |>
  tidyr::pivot_wider(
    id_cols = c("task_label"), 
    names_from = "learner_id", 
    names_sort = TRUE,
    values_from = fmt
  ) |>
  arrange(task_label) |>
  rename(Task = task_label) |>
  kableExtra::kable(
    caption = "Median (min - max) number of tuning evaluations per learner and task. (-) denotes a constant number of evaluations across resampling iterations.",
    escape = FALSE
  ) |>
  kableExtra::kable_styling() |>
  kableExtra::column_spec(1, width = "5cm")
```


```{r fun-plot_archive}
plot_archive <- function(xdf, param, log_y = FALSE, log_x = FALSE, outlier_threshold_percentile = 0.9) {
  
  p <- xdf |>
    filter(rmse <= quantile(rmse, probs = outlier_threshold_percentile), .by = "task_id") |>
    mutate(task_label = reorder(task_label, dim)) |>
    ggplot(aes(x = .data[[param]], y = rmse, color = learner_id, fill = learner_id)) +
    facet_wrap(vars(task_label), scales = "free", ncol = 4) +
    geom_point(size = 2, alpha = .1, shape = 21, key_glyph = "rect") +
    geom_smooth(se = FALSE, show.legend = FALSE, method = "lm") +
    scale_color_brewer(palette = "Dark2", labels = learner_label, aesthetics = c("color", "fill")) +
    guides(fill = guide_legend(override.aes = list(alpha = .8))) +
    labs(
      title = glue::glue("Tuning results for param `{param}`"),
      subtitle = "Result of up to 200 evaluations of Bayesian optimization",
      y = "RMSE", color = NULL, fill = NULL,
      caption = glue::glue("Removed outliers above {scales::ordinal(100 * outlier_threshold_percentile)} percentile")
    ) +
    theme_minimal() +
    theme(
      panel.spacing.x = unit(5, "mm"),
      plot.title.position = "plot"
    )
  
  if (log_x) {
    p <- p + scale_x_log10(labels = scales::label_comma())
  } else {
    p <- p + scale_x_continuous(breaks = scales::pretty_breaks(), labels = scales::label_comma())
  }
  
  if (log_y) {
    p <- p + scale_y_log10(labels = scales::label_comma())
  } else {
    p <- p + scale_y_continuous(labels = scales::label_comma())
  }
  
  if (length(unique(xdf$learner_id)) == 1) {
    p + theme(legend.position = "none")
  } else {
    p + theme(legend.position = "top")
  }
}
```

#### Results

Showing only the best performing parameter combinations of the inner folds, e.g. the parameters that "won".
One plot per hyperparameter, with performance on y-axis.

Note that `max_interaction` is tuned by tuning `max_interaction_ratio` within \[0, 1\] and then calculating

$$\max(\lceil \mathtt{max\_interaction\_ratio} \cdot \min(p, 20)\rceil, 1)$$

to effectively tune `max_interaction` within 1 and $p$ or 20, whichever is lower.
This is not ideal since multiple values for `max_interaction_ratio` could result in the same value for `max_interaction` for small $p$, but it's a compromise.

```{r tuning-results}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#| message: false
#| error: false
#| warning: false
# column: screen-inset-shaded

plot_archive(results_rpf, "splits")
plot_archive(results_rpf, "split_try")
plot_archive(results_rpf, "t_try")
plot_archive(results_rpf, "max_interaction")
```

#### Winning `splits`

```{r tbl-tuning-results-rpf-splits-tab}
results_rpf[, .(task_id, splits)][, .(median_splits = median(splits)), by = .(task_id)][task_meta, on = "task_id"] |>
  arrange(dim_rank) |>
  select(task_id, n, p, dim_rank, median_splits) |>
  kableExtra::kbl(
    caption = "Winning number of splits per task, median over resampling iterations"
  ) |>
  kableExtra::kable_styling()
```


#### Tuning Archives

Showing all evaluated hyperparameter configurations to get an impression of the space the MBO algorithm explored.



##### RPF / RPF (2)

```{r tuning-archives-rpf, eval=knitr::is_html_output()}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#| message: false
#| error: false
#| warning: false
#| cache: false
# column: screen-inset-shaded

plot_archive(archives_rpf, "splits")
plot_archive(archives_rpf, "split_try")
plot_archive(archives_rpf, "t_try")
plot_archive(archives_rpf, "max_interaction")
```

##### XGBoost / XGBoost (2)

```{r tuning-archives-xgb, eval=knitr::is_html_output()}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#| message: false
#| error: false
#| warning: false
#| cache: true
# column: screen-inset-shaded

plot_archive(archives_xgb, "max_depth"       , log_y = FALSE, log_x = FALSE)
plot_archive(archives_xgb, "subsample"       , log_y = FALSE, log_x = FALSE)
plot_archive(archives_xgb, "colsample_bytree", log_y = FALSE, log_x = FALSE)
plot_archive(archives_xgb, "eta"             , log_y = FALSE, log_x = FALSE)
plot_archive(archives_xgb, "nrounds"         , log_y = FALSE, log_x = FALSE)
```

##### ranger

```{r tuning-archives-ranger, eval=knitr::is_html_output()}
#| fig-width: 13
#| fig-height: 11
#| fig-align: center
#| message: false
#| error: false
#| warning: false
#| cache: true
# column: screen-inset-shaded

plot_archive(archives_ranger, "mtry"           , log_y = FALSE, log_x = FALSE)
plot_archive(archives_ranger, "min.node.size"  , log_y = FALSE, log_x = FALSE)
plot_archive(archives_ranger, "sample.fraction", log_y = FALSE, log_x = FALSE)

```

## Citation

```{r citation}
packages <- c("mlr3", "mlr3learners", "mlr3extralearners", "mlr3mbo", "randomPlantedForest", "xgboost", "ranger", "batchtools")

knitr::write_bib(x = packages, prefix = "pkg-", file = "packages.bib")

sapply(packages, \(x) {
  sprintf("\\emph{%s} v%s \\cite{pkg-%s}", x, packageVersion(x), x)
}) |>
  paste(collapse = ", ") |>
  cat()
```
